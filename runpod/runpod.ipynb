{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "IxyMPJBGzhLF",
    "ExecuteTime": {
     "end_time": "2024-07-11T09:48:48.418124Z",
     "start_time": "2024-07-11T09:48:48.413584Z"
    }
   },
   "source": [
    "import requests\n",
    "import runpod\n",
    "#from text_generation import Client"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BXshcVfScXr1",
    "ExecuteTime": {
     "end_time": "2024-07-11T11:34:25.414095Z",
     "start_time": "2024-07-11T11:34:24.636288Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "runpod.api_key = os.environ['RUNPOD_API_KEY']\n",
    "runpod.get_gpus()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'AMD Instinct MI300X OAM', 'displayName': 'MI300X', 'memoryInGb': 192},\n",
       " {'id': 'NVIDIA A100 80GB PCIe', 'displayName': 'A100 PCIe', 'memoryInGb': 80},\n",
       " {'id': 'NVIDIA A100-SXM4-80GB', 'displayName': 'A100 SXM', 'memoryInGb': 80},\n",
       " {'id': 'NVIDIA A30', 'displayName': 'A30', 'memoryInGb': 24},\n",
       " {'id': 'NVIDIA A40', 'displayName': 'A40', 'memoryInGb': 48},\n",
       " {'id': 'NVIDIA GeForce RTX 3070', 'displayName': 'RTX 3070', 'memoryInGb': 8},\n",
       " {'id': 'NVIDIA GeForce RTX 3080',\n",
       "  'displayName': 'RTX 3080',\n",
       "  'memoryInGb': 10},\n",
       " {'id': 'NVIDIA GeForce RTX 3080 Ti',\n",
       "  'displayName': 'RTX 3080 Ti',\n",
       "  'memoryInGb': 12},\n",
       " {'id': 'NVIDIA GeForce RTX 3090',\n",
       "  'displayName': 'RTX 3090',\n",
       "  'memoryInGb': 24},\n",
       " {'id': 'NVIDIA GeForce RTX 3090 Ti',\n",
       "  'displayName': 'RTX 3090 Ti',\n",
       "  'memoryInGb': 24},\n",
       " {'id': 'NVIDIA GeForce RTX 4070 Ti',\n",
       "  'displayName': 'RTX 4070 Ti',\n",
       "  'memoryInGb': 12},\n",
       " {'id': 'NVIDIA GeForce RTX 4080',\n",
       "  'displayName': 'RTX 4080',\n",
       "  'memoryInGb': 16},\n",
       " {'id': 'NVIDIA GeForce RTX 4090',\n",
       "  'displayName': 'RTX 4090',\n",
       "  'memoryInGb': 24},\n",
       " {'id': 'NVIDIA H100 80GB HBM3', 'displayName': 'H100 SXM', 'memoryInGb': 80},\n",
       " {'id': 'NVIDIA H100 NVL', 'displayName': 'H100 NVL', 'memoryInGb': 94},\n",
       " {'id': 'NVIDIA H100 PCIe', 'displayName': 'H100 PCIe', 'memoryInGb': 80},\n",
       " {'id': 'NVIDIA L4', 'displayName': 'L4', 'memoryInGb': 24},\n",
       " {'id': 'NVIDIA L40', 'displayName': 'L40', 'memoryInGb': 48},\n",
       " {'id': 'NVIDIA L40S', 'displayName': 'L40S', 'memoryInGb': 48},\n",
       " {'id': 'NVIDIA RTX 2000 Ada Generation',\n",
       "  'displayName': 'RTX 2000 Ada',\n",
       "  'memoryInGb': 16},\n",
       " {'id': 'NVIDIA RTX 4000 Ada Generation',\n",
       "  'displayName': 'RTX 4000 Ada',\n",
       "  'memoryInGb': 20},\n",
       " {'id': 'NVIDIA RTX 4000 SFF Ada Generation',\n",
       "  'displayName': 'RTX 4000 Ada SFF',\n",
       "  'memoryInGb': 20},\n",
       " {'id': 'NVIDIA RTX 5000 Ada Generation',\n",
       "  'displayName': 'RTX 5000 Ada',\n",
       "  'memoryInGb': 32},\n",
       " {'id': 'NVIDIA RTX 6000 Ada Generation',\n",
       "  'displayName': 'RTX 6000 Ada',\n",
       "  'memoryInGb': 48},\n",
       " {'id': 'NVIDIA RTX A2000', 'displayName': 'RTX A2000', 'memoryInGb': 6},\n",
       " {'id': 'NVIDIA RTX A4000', 'displayName': 'RTX A4000', 'memoryInGb': 16},\n",
       " {'id': 'NVIDIA RTX A4500', 'displayName': 'RTX A4500', 'memoryInGb': 20},\n",
       " {'id': 'NVIDIA RTX A5000', 'displayName': 'RTX A5000', 'memoryInGb': 24},\n",
       " {'id': 'NVIDIA RTX A6000', 'displayName': 'RTX A6000', 'memoryInGb': 48},\n",
       " {'id': 'Tesla V100-FHHL-16GB', 'displayName': 'V100 FHHL', 'memoryInGb': 16},\n",
       " {'id': 'Tesla V100-PCIE-16GB', 'displayName': 'Tesla V100', 'memoryInGb': 16},\n",
       " {'id': 'Tesla V100-SXM2-16GB', 'displayName': 'V100 SXM2', 'memoryInGb': 16},\n",
       " {'id': 'Tesla V100-SXM2-32GB',\n",
       "  'displayName': 'V100 SXM2 32GB',\n",
       "  'memoryInGb': 32},\n",
       " {'id': 'unknown', 'displayName': 'unknown', 'memoryInGb': 0}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8rH7Hfx_E3tP",
    "outputId": "56357af8-7460-4364-d0cd-1f40e840e893",
    "ExecuteTime": {
     "end_time": "2024-07-11T11:42:16.661137Z",
     "start_time": "2024-07-11T11:42:15.425915Z"
    }
   },
   "source": [
    "gpu_count = 1\n",
    "\n",
    "pod = runpod.create_pod(\n",
    "    name=\"Inference Server\",\n",
    "    image_name=\"ghcr.io/huggingface/text-generation-inference\",\n",
    "    gpu_type_id=\"NVIDIA RTX 6000 Ada Generation\",\n",
    "    #gpu_type_id=\"NVIDIA A40\",\n",
    "    # gpu_type_id=\"NVIDIA A100 80GB PCIe\",\n",
    "    #data_center_id=\"EU-NL-1\",\n",
    "    cloud_type=\"SECURE\",\n",
    "    #docker_args=\"--model-id TheBloke/Llama-2-7b-chat-fp16\",\n",
    "    #docker_args=\"--model-id HuggingFaceH4/zephyr-7b-beta\",\n",
    "    #docker_args=\"--model-id meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    # https://github.com/huggingface/text-generation-inference/blob/main/docs/source/installation_nvidia.md\n",
    "    # docker run --gpus all --shm-size 64g -p 8080:80 ghcr.io/huggingface/text-generation-inference:2.0.3 --model-id meta-llama/Meta-Llama-3-8B-Instruct\n",
    "    volume_in_gb=50,\n",
    "    container_disk_in_gb=5,\n",
    "    ports=\"80/http,29500/http,22/tcp\",\n",
    "    volume_mount_path=\"/data\",\n",
    "    start_ssh=\"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDCSJRuP843fTQKtKQrKTHX6mycgBJeflS77E/DauYxb7L9871Ntp/kwANq04jefwrRdkjsySnIFDbXzYf0irIMrFKgLRoFCgoa4E4yo+AjFYQVTD2T1AWGPKsFtKqaICznua4gvN79t0UBc7W8nofvTU+lvqiSaLmFNv+LbxMvxWCf8DtbehzE5UG7xu82EFOP4o7ONthj+9EBtYKxbD6gu5ivPyjU4dJbkDkVQvFouBXuphuLgqHAmfJo9rAcffAmm/y7mm8twbdcC6GSJzNivmAlfD8JgKiqS4g0HvZb37tINVUWTWBFtB7Yk4j2w/DO599T4rUqVIJITgz2nKMmxmJKEhYqLxAKHjmdeTZlC/HC7AH2U2HA6EI1h9hxf5CmDSes1gA4CHAcChw9IPW+gzhsMRKRhXPbNZ/XfRAPdcpr4qFucEWSM+J+lSDZ37JRyArEgogP2QrfxsG/LTowtyG8FpzZQFImMneLoQ3YZcI7lewjJ9n89xx8Kvqbe/0X5NgWwOhxw9jofH1A2LnpbZwwJqM5L3nnmCZ+yOPRWGZdYa0UFJ3DaemuNFHAlG58zg63BTKuDJg1vW113f7F0hOMWaLqOLxcDaGHr6MOFi+D08aZGS3Cn3cGf7qvMgRtqsqqZWbSo0zl5b9CDY1BRCCM3ru/wFm6g20y3DzxMQ== gp@Gians-MacBook-Pro.local\"\n",
    ")\n",
    "pod"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'lz4mb3r3t0t4kg',\n",
       " 'desiredStatus': 'RUNNING',\n",
       " 'imageName': 'ghcr.io/huggingface/text-generation-inference',\n",
       " 'env': ['PUBLIC_KEY=ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIFXunZywNXCo47gSNLcl0PfVmq6wXBqIcaIBupH90EVz gp@Gians-MacBook-Pro.local'],\n",
       " 'machineId': '28nwudf6774h',\n",
       " 'machine': {'podHostId': 'lz4mb3r3t0t4kg-644111cf'}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T11:34:38.388201Z",
     "start_time": "2024-07-11T11:34:38.385132Z"
    }
   },
   "source": [
    "# text-generation-launcher --model-id HuggingFaceH4/zephyr-7b-beta\n",
    "# https://www.youtube.com/watch?v=jlMAX2Oaht0\n",
    "# text-generation-benchmark --tokenizer-name HuggingFaceH4/zephyr-7b-beta   TheBloke/Llama-2-7b-chat-fp16\n",
    "# text-generation-benchmark --tokenizer-name HuggingFaceH4/zephyr-7b-beta  --batch-size 8 --batch-size 16 --batch-size 32 --batch-size 64 --batch-size 128 --batch-size 256\n",
    "# nvidia-smi"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LCHpvkP7xK-8",
    "outputId": "10010d02-7737-4e60-b51a-a3cc03e17225",
    "ExecuteTime": {
     "end_time": "2024-07-11T12:08:15.348611Z",
     "start_time": "2024-07-11T12:08:15.342722Z"
    }
   },
   "source": [
    "SERVER_URL = f'https://{pod[\"id\"]}-80.proxy.runpod.net'\n",
    "print(SERVER_URL)\n",
    "print(pod[\"id\"])\n",
    "TGI_URL = f'url https://{pod[\"id\"]}-80.proxy.runpod.net/generate     -X POST     -d '{\"inputs\":\"What is Deep Learning?\",\"parameters\":{\"max_new_tokens\":20}}'     -H 'Content-Type: application/json''\n",
    " "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://lz4mb3r3t0t4kg-80.proxy.runpod.net\n",
      "lz4mb3r3t0t4kg\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-kSXFcZMtlxT",
    "outputId": "21e17fa0-ce2d-4107-f0a3-abe116b25947",
    "ExecuteTime": {
     "end_time": "2024-07-11T11:16:38.553843Z",
     "start_time": "2024-07-11T11:16:38.549355Z"
    }
   },
   "source": [
    "print(f\"Docs (Swagger UI) URL: {SERVER_URL}/docs\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs (Swagger UI) URL: https://z829rdijoth63s-80.proxy.runpod.net/docs\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "owo7RuELe3aP",
    "ExecuteTime": {
     "end_time": "2024-07-11T09:28:12.631007Z",
     "start_time": "2024-07-11T09:28:12.054811Z"
    }
   },
   "source": [
    "runpod.terminate_pod(pod[\"id\"])"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1h1REGtGMc28"
   },
   "source": [
    "## References\n",
    "\n",
    "- https://www.runpod.io/console/gpu-secure-cloud\n",
    "- https://docs.runpod.io/docs/get-gpu-types\n",
    "- https://github.com/facebookresearch/llama"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
