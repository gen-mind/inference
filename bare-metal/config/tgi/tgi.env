#tgi
LOCAL_MODEL_CACHE_DIR= ../data/localcache # location on your host machine for the huggingface cache
HF_TOKEN= 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# if you are wanting to run llama2 from huggingface you will need a token
MODEL_ID=mistralai/Mistral-7B-Instruct-v0.2
QUANTIZATION=bitsandbytes-nf4
MAX_PREFILL_TOKENS=3072
MAX_TOTAL_TOKENS=40980
MAX_INPUT_LENGTH=3000