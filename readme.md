# ğŸš€ **Serving a Custom LLM for Multiple Users**

Whether you're serving **employees in your company** or **users of your app**, deploying a Large Language Model (LLM) requires choosing between two major options:

1. **SaaS Services** like OpenAI, Anthropic, and many others.  
   ğŸ’¼ *Pro*: No infrastructure needed, fast setup.  
   ğŸ› ï¸ *Con*: Dependency on external providers, pricing per usage.

2. **Inference Servers** capable of handling **multiple concurrent requests**.  
   ğŸŒ *Pro*: Full control over the deployment, cost-effective at scale.  
   âš™ï¸ *Con*: Infrastructure setup and maintenance.

---

## ğŸ” **This Repository**

As a community, we will collect and explore various approaches to serving LLMs in different environments, such as **on-premises** ğŸ¢ or even **air-gapped environments** ğŸ”’. Our goal is to collaboratively evaluate and share best practices.

- ğŸ’» [RunPod](./runpod/readme.md)  
- ğŸ–¥ï¸ [Bare metal (multi-GPU)](./bare-metalreadme.md)  
- âš™ï¸ [GPU virtual machines](./bare-metalreadme.md) 

---

## ğŸš€ **Performance Comparison**

We will compare the **performance** of multiple inference servers, including:
- ğŸ… **vLLM**  
- ğŸš€ **TGI**  
- ğŸ’¡ **SGlang**  
- ğŸ’» **Aphrodite**

---

## ğŸ›ï¸ **Inference UIs**

We will also explore different UIs for serving LLMs:
- ğŸŒ **AnythingLLM**

---

## ğŸ§® **Quantization**: What, When, and Why?

**Quantization** reduces the precision of model weights (e.g., from FP32 to INT8) to make the model more lightweight and faster. But when should you use it?

âœ… **When to use**:  
- ğŸ–¥ï¸ Low-memory environments  
- âš¡ Need for speed and efficiency

âŒ **When NOT to use**:  
- ğŸ¯ If precision is critical for your application  
- ğŸ”¬ For tasks requiring fine-tuned model accuracy

---

## ğŸ‰ **Join the Community!**

This repository is a community effort, and we invite contributions, discussions, and ideas from everyone interested in serving LLMs! ğŸ¤ Together, we will explore and document different configurations and approaches to help the entire community grow.

Stay tuned as we collaborate on this journey! ğŸ˜„






